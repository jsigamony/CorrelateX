# -*- coding: utf-8 -*-
"""PyTorch2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/15dczhXMdPoZumPKD6hWSa-fazQsgl54-

## PyTorch Workflow

**Exploring PyTorch end to end**
"""

import torch

from torch import nn # nn contains all of PyTorch's building block for creating neural networks

import matplotlib.pyplot as plt

#Checking pytorch version

torch.__version__

"""## Preparing the data for our model"""

#Creating parameters for the

# y = a + bX

weight = 0.7 #b
bias = 0.3 #a

#Create range of numbers
start = 0
end = 1
step = 0.02

#capital represesnts a vector/matrix so we denote x with capital
X = torch.arange(start, end, step).unsqueeze(dim=1)
Y = weight * X + bias

X[:10], Y[:10]

"""#total length of X and Y is 50
len(X), len(Y)

# X(independent variable) is input numbers and Y(dependent variable) = output numbers

## Splitting the data set into testing and training set and validation set (one of the most important concept in machine learning)

* Majority of the data set is training set (80%)
* 20% for the testing set
"""

training_split = int(0.8 * len(X))

X_train, Y_train = X[:training_split], Y[:training_split]

X_test, Y_test = X[training_split:], Y[training_split:]

len(X_train), len(Y_train), len(X_test), len(Y_test)

X_test

def plot_predictions(train_data=X_train,
                     train_labels=Y_train,
                     test_data=X_test,
                     test_labels = Y_test,
                     predictions=None):

  """
  Plots training data, test data and compares predictions.
  """
  plt.figure(figsize=(10,7))

  # Plot training data in blue
  plt.scatter(train_data, train_labels, c="b", s=4, label="Training data")

  """plotting the test data (x-values) and test_labels(y-values) before hand
  """
  plt.scatter(test_data, test_labels, c="g", s=4, label="Testing data")

  #Checking if there are predictions made
  if predictions is not None:
    #Plot the predictions if they exist

    #Plotting the predicted y values(predictions) against the test x-values
    plt.scatter(test_data, predictions, c="r", s=8, label="Predictions")

    #Show the legend
  plt.legend(prop={"size": 14})

plot_predictions()

"""## Building Model (Linear Regression Model)

What our model does is:
* it starts with random values (weight and bias)
* it thens looks at training data and adjust the random values to better represent the ideal values

Algorithms used:
1. Gradient descent
2. Backpropogation
"""

# Creating a linear regression model class

#everythin in pytorch uses nn module
class LinearRegressionModel(nn.Module):
  def __init__(self):
    super().__init__()
    self.weights = nn.Parameter(torch.randn(1,
                                            requires_grad=True,
                                            dtype=torch.float))
    self.bias = nn.Parameter(torch.randn(1,
                                         requires_grad=True,
                                         dtype=torch.float))

  # Forward method to define the computation in the model
  def forward(self, x:torch.Tensor) -> torch.Tensor: #"x" is the input data
    return self.weights * x + self.bias #linear regression formula

"""## PyTorch model building essentials

* torch.nn - contains all building blocks for copmutational graphs
"""

# Checking the contents of the PyTorch model

# Creating a random seed

torch.manual_seed(42)

# Creating an instance of the model class (subclass of nn.Module)

model0 = LinearRegressionModel()

#checking out the parameters of model 0
# .parameters() returns a generate (iterator) object

list(model0.parameters())

# for i in model0.parameters():
#   print(i)

#List named parameters
print(model0.state_dict())

print(model0.weights, model0.bias)

# Making prediction using torch.inference_mode()

# Both do the same thing but inference mode is preferred

with torch.inference_mode():
  y_preds = model0.forward(X_test)

# y_preds = model0.forward(X_test)

# with torch.no_grad():
#   y_preds = model0(X_test)


y_preds

plot_predictions(predictions=y_preds)

"""## Training the model

One way to measure how poor or how wrong our models predictions are is to use a loss function

* Note: Loss function is also called cost function or criterion in different areas.

* **Loss function:** A function to measure how wrong your model's predictions are to the ideal outputs, lower is better.
* **Optimizer:** Takes into account the loss of a model and adjusts the model's parameteres (e.g weight & bias) to improve the loss function.






For PyTorch, we need:
* A training loop
* A testing loop
"""

#Setting up a loss function

loss_fn = nn.L1Loss()

#Setting up an optimizer (SGD - stochastic gradient descent)
optimizer = torch.optim.SGD(params=model0.parameters(), #we want to optimize the parameters of our model0
                            lr=0.01) #lr = learning rate

"""## Building a training loop(and a testing loop) in PyTorch

Things we need in a training loop:
0. Loop thru the data and...
1. Forward pass (this involves data moving thru our model's `forward()` functions) to make predictions on data
2. Calculate the loss  (compare forward pass predictions to ground truth labels)
3. Optimizer zero grad
4. Loss backward - move backwards through the neural network to calculate the gradients of each of the parameters of our model with respect to the loss (**backpropogation**)
5. Optimizer step - use the optimizer to adjust our model's parameters to try and improve the loss (**gradient descent**)
"""

with torch.inference_mode():
  print(list(model0.parameters()))

torch.manual_seed(42)

# An epoch is one loop through the data...
epochs = 100

# 0. Loop thru the data
for epoch in range(epochs):
  # Set the model to training mode
  model0.train()

  #1. Forward pass
  y_pred = model0(X_train)

  #2. Calculate the loss
  loss = loss_fn(y_pred, Y_train)
  print(f"Loss:{loss}")

  #3. Optimizer zero grad
  optimizer.zero_grad()

  #4. Perform backpropogation on the loss with respect to the parameters of the model
  loss.backward()

  #5. Step the optimizer (perform gradient descent)
  optimizer.step() #by default how the optimizer changes will accumulate thru the loop


  # Testing
  model0.eval() # turns off different settings in the model not needed for evaluation/testing (dropout/batch norm layers)

  with torch.inference_mode():
    # 1. Forward pass
    test_pred = model0(X_test)

    #2. Calculate the loss
    test_loss = loss_fn(test_pred, Y_test)

  # Print out what's happening
  if epoch % 10 == 0:
    print(f"Epoch: {epoch} | Test: {test_pred} | Test loss: {test_loss}")

    #Printing parameters
    print(model0.state_dict())

model0.state_dict()

with torch.inference_mode():
  y_preds_new = model0(X_test)

plot_predictions(predictions=y_preds_new)

"""## Saving a model in PyTorch

Three main methods I should know about for saving and loading models in PyTorch

1. `torch.save()` - allows you to save a PyTorch object in Python's pickle format
2. `torch.load()` - allows you to load a saved PyTorch object
3. `torch.nn.Module.load_state_dict()` - this allows to load a model's saved state dictionary


"""

# Saving our PyTorch model

from pathlib import Path

#1. Create model's directory
MODEL_PATH = Path("models")

MODEL_PATH.mkdir(parents=True, exist_ok=True)

#2. Create model save path
MODEL_NAME = "01_linearRegModel.pth"
MODEL_SAVE_PATH = MODEL_PATH / MODEL_NAME

MODEL_SAVE_PATH


#3. Save the model's state dict
print(f"Saving model to: {MODEL_SAVE_PATH}")

torch.save(obj=model0.state_dict(),
           f= MODEL_SAVE_PATH)

"""# To load in a saved state_dict we have to instantiate a new instance of our model class

"""

load_model0 = LinearRegressionModel()

load_model0.load_state_dict(torch.load(f=MODEL_SAVE_PATH))

load_model0.state_dict()

#Making some predictions with loaded model

with torch.inference_mode():
  load_model_preds = load_model0(X_test)


load_model_preds == y_preds_new